{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['CreditScore', 'Age' , 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']]\n",
    "\n",
    "geo = pd.get_dummies(dataset['Geography'], drop_first=True)\n",
    "gender = pd.get_dummies(dataset['Gender'], drop_first=True)\n",
    "\n",
    "#to save it from dummy variables trap, we always drop one column\n",
    "\n",
    "X = pd.concat([X, geo, gender], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset[['Exited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first layer: Hidden layer\n",
    "\n",
    "#input_dim = 11 (means 11 features in my X variable)\n",
    "\n",
    "model.add(Dense(units=8, activation = 'relu', input_dim=11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 96        \n",
      "=================================================================\n",
      "Total params: 96\n",
      "Trainable params: 96\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 96        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "=================================================================\n",
      "Total params: 240\n",
      "Trainable params: 240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 96        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "=================================================================\n",
      "Total params: 512\n",
      "Trainable params: 512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 96        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "=================================================================\n",
      "Total params: 784\n",
      "Trainable params: 784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last layer will always have sigmoid function\n",
    "#sigmoid function -  works best for binary classification (0 & 1)\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer = 'glorot_uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 96        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 801\n",
      "Trainable params: 801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ankit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate = 0.000001))\n",
    "\n",
    "#learning_rate => hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ankit/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "8000/8000 [==============================] - 1s 150us/step - loss: 2391.4344\n",
      "Epoch 2/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2268.7678\n",
      "Epoch 3/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2148.2700\n",
      "Epoch 4/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 2030.7024\n",
      "Epoch 5/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1929.9900\n",
      "Epoch 6/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 1852.3064\n",
      "Epoch 7/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 1780.6839\n",
      "Epoch 8/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 1710.2177\n",
      "Epoch 9/200\n",
      "8000/8000 [==============================] - 0s 50us/step - loss: 1639.2463\n",
      "Epoch 10/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1567.4389\n",
      "Epoch 11/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 1495.4385\n",
      "Epoch 12/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1423.1502\n",
      "Epoch 13/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 1350.8271\n",
      "Epoch 14/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1278.8153\n",
      "Epoch 15/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1207.0850\n",
      "Epoch 16/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 1136.4953\n",
      "Epoch 17/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 1069.4568\n",
      "Epoch 18/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 1006.6104\n",
      "Epoch 19/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 948.0152\n",
      "Epoch 20/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 894.9220\n",
      "Epoch 21/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 846.9348\n",
      "Epoch 22/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 803.6446\n",
      "Epoch 23/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 764.6308\n",
      "Epoch 24/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 728.9661\n",
      "Epoch 25/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 694.9671\n",
      "Epoch 26/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 662.7834\n",
      "Epoch 27/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 632.3411\n",
      "Epoch 28/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 602.9346\n",
      "Epoch 29/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 574.5652\n",
      "Epoch 30/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 547.1941\n",
      "Epoch 31/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 520.4590\n",
      "Epoch 32/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 494.4465\n",
      "Epoch 33/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 469.8183\n",
      "Epoch 34/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 447.6996\n",
      "Epoch 35/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 425.9294\n",
      "Epoch 36/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 404.1206\n",
      "Epoch 37/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 382.3005\n",
      "Epoch 38/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 360.4508\n",
      "Epoch 39/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 338.6447\n",
      "Epoch 40/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 316.7460\n",
      "Epoch 41/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 294.7436\n",
      "Epoch 42/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 272.7505\n",
      "Epoch 43/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 250.8133\n",
      "Epoch 44/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 228.7753\n",
      "Epoch 45/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 206.7250\n",
      "Epoch 46/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 184.6321\n",
      "Epoch 47/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 165.7067\n",
      "Epoch 48/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 153.4895\n",
      "Epoch 49/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 141.7680\n",
      "Epoch 50/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 132.2150\n",
      "Epoch 51/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 124.9141\n",
      "Epoch 52/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 118.4518\n",
      "Epoch 53/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 112.2158\n",
      "Epoch 54/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 106.4079\n",
      "Epoch 55/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 101.0727\n",
      "Epoch 56/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 95.9296\n",
      "Epoch 57/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 90.8692\n",
      "Epoch 58/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 85.8904\n",
      "Epoch 59/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 80.9755\n",
      "Epoch 60/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 76.0751\n",
      "Epoch 61/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 71.1832\n",
      "Epoch 62/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 66.3415\n",
      "Epoch 63/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 61.5807\n",
      "Epoch 64/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 56.9054\n",
      "Epoch 65/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 52.2831\n",
      "Epoch 66/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 47.7380\n",
      "Epoch 67/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 44.7762\n",
      "Epoch 68/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 43.7032\n",
      "Epoch 69/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.9249\n",
      "Epoch 70/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 42.2528\n",
      "Epoch 71/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 41.6196\n",
      "Epoch 72/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 41.0000\n",
      "Epoch 73/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 40.3954\n",
      "Epoch 74/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.7919\n",
      "Epoch 75/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 39.2451\n",
      "Epoch 76/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.6774\n",
      "Epoch 77/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 38.1253\n",
      "Epoch 78/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.5881\n",
      "Epoch 79/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 37.0554\n",
      "Epoch 80/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 36.5374\n",
      "Epoch 81/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 36.0159\n",
      "Epoch 82/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 35.5289\n",
      "Epoch 83/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 35.0226\n",
      "Epoch 84/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 34.5433\n",
      "Epoch 85/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 34.0475\n",
      "Epoch 86/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 33.5636\n",
      "Epoch 87/200\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 33.0696\n",
      "Epoch 88/200\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 32.5931\n",
      "Epoch 89/200\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 32.1118\n",
      "Epoch 90/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 31.6231\n",
      "Epoch 91/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 31.1686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 30.6991\n",
      "Epoch 93/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 30.2385\n",
      "Epoch 94/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.7810\n",
      "Epoch 95/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 29.3356\n",
      "Epoch 96/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.8774\n",
      "Epoch 97/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 28.4392\n",
      "Epoch 98/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 28.0003\n",
      "Epoch 99/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 27.5960\n",
      "Epoch 100/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 27.1818\n",
      "Epoch 101/200\n",
      "8000/8000 [==============================] - 0s 51us/step - loss: 26.7852\n",
      "Epoch 102/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 26.3859\n",
      "Epoch 103/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 25.9932\n",
      "Epoch 104/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.5960\n",
      "Epoch 105/200\n",
      "8000/8000 [==============================] - 0s 52us/step - loss: 25.2038\n",
      "Epoch 106/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 24.8119\n",
      "Epoch 107/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 24.4103\n",
      "Epoch 108/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 24.0202\n",
      "Epoch 109/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 23.6230\n",
      "Epoch 110/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 23.2288\n",
      "Epoch 111/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.8330\n",
      "Epoch 112/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 22.4642\n",
      "Epoch 113/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 22.0604\n",
      "Epoch 114/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 21.6728\n",
      "Epoch 115/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 21.2955\n",
      "Epoch 116/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 20.9358\n",
      "Epoch 117/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 20.5936\n",
      "Epoch 118/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 20.2666\n",
      "Epoch 119/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 19.9669\n",
      "Epoch 120/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.6817\n",
      "Epoch 121/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.4276\n",
      "Epoch 122/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 19.1653\n",
      "Epoch 123/200\n",
      "8000/8000 [==============================] - 0s 54us/step - loss: 18.9160\n",
      "Epoch 124/200\n",
      "8000/8000 [==============================] - 0s 53us/step - loss: 18.6869\n",
      "Epoch 125/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.4801\n",
      "Epoch 126/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 18.2636\n",
      "Epoch 127/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 18.0618\n",
      "Epoch 128/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 17.8666\n",
      "Epoch 129/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 17.6817\n",
      "Epoch 130/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.4904\n",
      "Epoch 131/200\n",
      "8000/8000 [==============================] - 0s 56us/step - loss: 17.3195\n",
      "Epoch 132/200\n",
      "8000/8000 [==============================] - 0s 55us/step - loss: 17.1545\n",
      "Epoch 133/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 16.9854\n",
      "Epoch 134/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 16.8253\n",
      "Epoch 135/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 16.6801\n",
      "Epoch 136/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 16.5350\n",
      "Epoch 137/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 16.3856\n",
      "Epoch 138/200\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 16.2587\n",
      "Epoch 139/200\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 16.1206\n",
      "Epoch 140/200\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 15.9833\n",
      "Epoch 141/200\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 15.8572\n",
      "Epoch 142/200\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 15.7299\n",
      "Epoch 143/200\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 15.6071\n",
      "Epoch 144/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.4773\n",
      "Epoch 145/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 15.3506\n",
      "Epoch 146/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 15.2459\n",
      "Epoch 147/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 15.1223\n",
      "Epoch 148/200\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 15.0025\n",
      "Epoch 149/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 14.8844\n",
      "Epoch 150/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 14.7729\n",
      "Epoch 151/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 14.6711\n",
      "Epoch 152/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 14.5584\n",
      "Epoch 153/200\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 14.4458\n",
      "Epoch 154/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 14.3297\n",
      "Epoch 155/200\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 14.2229\n",
      "Epoch 156/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 14.1244\n",
      "Epoch 157/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 14.0138\n",
      "Epoch 158/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 13.9040\n",
      "Epoch 159/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 13.8044\n",
      "Epoch 160/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 13.6973\n",
      "Epoch 161/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 13.6032\n",
      "Epoch 162/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.4943\n",
      "Epoch 163/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 13.3955\n",
      "Epoch 164/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.2989\n",
      "Epoch 165/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 13.1967\n",
      "Epoch 166/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 13.1182\n",
      "Epoch 167/200\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 13.0155\n",
      "Epoch 168/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 12.9168\n",
      "Epoch 169/200\n",
      "8000/8000 [==============================] - 1s 71us/step - loss: 12.8293\n",
      "Epoch 170/200\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 12.7406\n",
      "Epoch 171/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 12.6533\n",
      "Epoch 172/200\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 12.5625\n",
      "Epoch 173/200\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 12.4728\n",
      "Epoch 174/200\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.3769\n",
      "Epoch 175/200\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 12.3018\n",
      "Epoch 176/200\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 12.2144\n",
      "Epoch 177/200\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 12.1453\n",
      "Epoch 178/200\n",
      "8000/8000 [==============================] - 1s 68us/step - loss: 12.0559\n",
      "Epoch 179/200\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 11.9668\n",
      "Epoch 180/200\n",
      "8000/8000 [==============================] - 1s 69us/step - loss: 11.9002\n",
      "Epoch 181/200\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 11.8191\n",
      "Epoch 182/200\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 11.7382\n",
      "Epoch 183/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 11.6533\n",
      "Epoch 184/200\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 11.5798\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 63us/step - loss: 11.5118\n",
      "Epoch 186/200\n",
      "8000/8000 [==============================] - 0s 57us/step - loss: 11.4243\n",
      "Epoch 187/200\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 11.3512\n",
      "Epoch 188/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 11.2718\n",
      "Epoch 189/200\n",
      "8000/8000 [==============================] - 0s 59us/step - loss: 11.2000\n",
      "Epoch 190/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 11.1227\n",
      "Epoch 191/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 11.0596\n",
      "Epoch 192/200\n",
      "8000/8000 [==============================] - 0s 61us/step - loss: 10.9821\n",
      "Epoch 193/200\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 10.9007\n",
      "Epoch 194/200\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 10.8352\n",
      "Epoch 195/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 10.7643\n",
      "Epoch 196/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 10.6948\n",
      "Epoch 197/200\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 10.6221\n",
      "Epoch 198/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 10.5490\n",
      "Epoch 199/200\n",
      "8000/8000 [==============================] - 0s 58us/step - loss: 10.4862\n",
      "Epoch 200/200\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 10.4099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ffa91b836d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2391.4343666992186,\n",
       "  2268.76783984375,\n",
       "  2148.2700258789064,\n",
       "  2030.7023642578124,\n",
       "  1929.9900485839844,\n",
       "  1852.3063701171875,\n",
       "  1780.6838520507813,\n",
       "  1710.2176533203126,\n",
       "  1639.2463095703124,\n",
       "  1567.4389047851562,\n",
       "  1495.4384938964845,\n",
       "  1423.1502263183593,\n",
       "  1350.8270915527344,\n",
       "  1278.815343017578,\n",
       "  1207.08496875,\n",
       "  1136.4952612304687,\n",
       "  1069.456772705078,\n",
       "  1006.6103557128906,\n",
       "  948.0152003173828,\n",
       "  894.921980102539,\n",
       "  846.9348024902343,\n",
       "  803.6445661621094,\n",
       "  764.6307743530274,\n",
       "  728.9661107177734,\n",
       "  694.9671320800782,\n",
       "  662.7833911132813,\n",
       "  632.3411436767578,\n",
       "  602.9346079101563,\n",
       "  574.5652073364258,\n",
       "  547.1941121826172,\n",
       "  520.4590062255859,\n",
       "  494.44649743652343,\n",
       "  469.81832708740234,\n",
       "  447.6995737304687,\n",
       "  425.92942407226565,\n",
       "  404.12063006591796,\n",
       "  382.3004997558594,\n",
       "  360.45075646972657,\n",
       "  338.6446766662598,\n",
       "  316.7460301208496,\n",
       "  294.74358502197265,\n",
       "  272.7504861755371,\n",
       "  250.81333917236327,\n",
       "  228.7752593536377,\n",
       "  206.72501347351073,\n",
       "  184.63208784484863,\n",
       "  165.70667039108275,\n",
       "  153.4895153565407,\n",
       "  141.76796706235407,\n",
       "  132.2150345802307,\n",
       "  124.91410576128959,\n",
       "  118.45175193762779,\n",
       "  112.2158127439022,\n",
       "  106.40786524653434,\n",
       "  101.07273585677147,\n",
       "  95.92960514926911,\n",
       "  90.86924653434754,\n",
       "  85.89036943340301,\n",
       "  80.97547947502136,\n",
       "  76.07508342552185,\n",
       "  71.18322907829284,\n",
       "  66.34146276664734,\n",
       "  61.580675024032594,\n",
       "  56.90538723182678,\n",
       "  52.283085911035535,\n",
       "  47.738046703338625,\n",
       "  44.776193879127504,\n",
       "  43.70322041893005,\n",
       "  42.92491064167023,\n",
       "  42.25280396175385,\n",
       "  41.61963868713379,\n",
       "  41.00000545239448,\n",
       "  40.39541041254997,\n",
       "  39.79194814062119,\n",
       "  39.245080516815186,\n",
       "  38.67744547796249,\n",
       "  38.12528624153137,\n",
       "  37.5881111946106,\n",
       "  37.05535124111176,\n",
       "  36.53741523504257,\n",
       "  36.01590012979507,\n",
       "  35.52894582986832,\n",
       "  35.02258611917496,\n",
       "  34.543315147280694,\n",
       "  34.04752948570251,\n",
       "  33.56361208772659,\n",
       "  33.0696418864727,\n",
       "  32.59310398173332,\n",
       "  32.11175661706925,\n",
       "  31.623144449234008,\n",
       "  31.168594725608827,\n",
       "  30.69908056306839,\n",
       "  30.238521436214448,\n",
       "  29.780978456020357,\n",
       "  29.335585486412047,\n",
       "  28.87742774248123,\n",
       "  28.439209625959396,\n",
       "  28.000252820014953,\n",
       "  27.59595954334736,\n",
       "  27.181834105134012,\n",
       "  26.785183981895447,\n",
       "  26.385881550312043,\n",
       "  25.99321445119381,\n",
       "  25.596002460837365,\n",
       "  25.203832530260087,\n",
       "  24.811903400182725,\n",
       "  24.410319974184038,\n",
       "  24.020187759399413,\n",
       "  23.623048998236655,\n",
       "  23.228803897976874,\n",
       "  22.832983321666717,\n",
       "  22.46415251684189,\n",
       "  22.060394619464873,\n",
       "  21.672838491916657,\n",
       "  21.295450156450272,\n",
       "  20.9358419393301,\n",
       "  20.59360631132126,\n",
       "  20.266625597417356,\n",
       "  19.966905055880545,\n",
       "  19.681665671110153,\n",
       "  19.427587561726572,\n",
       "  19.165311856389046,\n",
       "  18.915976319789888,\n",
       "  18.68685127723217,\n",
       "  18.480130542874335,\n",
       "  18.26356940162182,\n",
       "  18.06183836746216,\n",
       "  17.86658973348141,\n",
       "  17.681718132019043,\n",
       "  17.49044864654541,\n",
       "  17.31951787376404,\n",
       "  17.154464055657385,\n",
       "  16.985394064664842,\n",
       "  16.825273429393768,\n",
       "  16.680135221362114,\n",
       "  16.53501687216759,\n",
       "  16.385643413662912,\n",
       "  16.258694927215576,\n",
       "  16.12058115673065,\n",
       "  15.983333499670028,\n",
       "  15.857173844575883,\n",
       "  15.729926157593727,\n",
       "  15.607086973667144,\n",
       "  15.477251127839088,\n",
       "  15.35055201280117,\n",
       "  15.2459422403574,\n",
       "  15.1223404327631,\n",
       "  15.00249962592125,\n",
       "  14.884382181048393,\n",
       "  14.772899955034257,\n",
       "  14.67110403394699,\n",
       "  14.558408271312713,\n",
       "  14.445776226162911,\n",
       "  14.329736005663872,\n",
       "  14.222878980875015,\n",
       "  14.124365489125251,\n",
       "  14.013800069093705,\n",
       "  13.90402532863617,\n",
       "  13.80437003648281,\n",
       "  13.697336941123009,\n",
       "  13.603220701932907,\n",
       "  13.494326313614845,\n",
       "  13.395546349406242,\n",
       "  13.29887007200718,\n",
       "  13.196683129191399,\n",
       "  13.118197902083397,\n",
       "  13.015495866775513,\n",
       "  12.916845885396004,\n",
       "  12.829256068468094,\n",
       "  12.74064440202713,\n",
       "  12.653292171835899,\n",
       "  12.562486253142357,\n",
       "  12.472802880883217,\n",
       "  12.376906988024711,\n",
       "  12.301825904250144,\n",
       "  12.214449114203454,\n",
       "  12.145338331222534,\n",
       "  12.055941179156303,\n",
       "  11.966840708494187,\n",
       "  11.900206406712533,\n",
       "  11.819121571302414,\n",
       "  11.738201951503754,\n",
       "  11.6533367125988,\n",
       "  11.579786899089813,\n",
       "  11.511806142449378,\n",
       "  11.424268519878387,\n",
       "  11.351151780366898,\n",
       "  11.271824734449387,\n",
       "  11.200033079862594,\n",
       "  11.12272033905983,\n",
       "  11.059641399741173,\n",
       "  10.982128390550614,\n",
       "  10.900675315499306,\n",
       "  10.835167537927628,\n",
       "  10.764346943378449,\n",
       "  10.694812810063363,\n",
       "  10.62210111105442,\n",
       "  10.548962419748307,\n",
       "  10.486196437954902,\n",
       "  10.409874304056167]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history\n",
    "#shows all the loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa67778090>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZhcdZ3v8fe3qpcs3Vl6yb52J0DCFkITSIJBB0mAq0ZkdGAUonKNjssjVwcvyr0DozMD4zrqo4wgaJirAjOgRkGRRYYd0wkJSUggnU4gnXSS7ux7L/W9f9TppJL0vtSprvN5PU89depX59T51unqzzn1O6fOMXdHRESiIRZ2ASIikj4KfRGRCFHoi4hEiEJfRCRCFPoiIhGi0BcRiZAOQ9/MxpvZn81snZmtNbMvBu13mNlWM1sZ3K5OmearZlZlZm+a2YKU9iuDtiozu7Vv3pKIiLTFOjpO38xGA6PdfYWZFQLLgQ8CHwEOuvu3Txl/OvArYBYwBngKOCN4+i3gCqAGWAZc7+5v9N7bERGR9uR0NIK71wK1wfABM1sHjG1nkoXAg+5+DNhkZlUkVwAAVe5eDWBmDwbjthn6JSUlPmnSpM68DxERCSxfvrze3Utbe67D0E9lZpOAC4BXgbnA583sRqAS+LK77yG5QnglZbIaTqwktpzSfnEr81gMLAaYMGEClZWVXSlRRCTyzOzttp7r9I5cMysAHgFudvf9wN1AOTCD5DeB77SM2srk3k77yQ3u97h7hbtXlJa2uqISEZFu6tSWvpnlkgz8X7j7owDuviPl+XuB3wcPa4DxKZOPA7YFw221i4hIGnTm6B0D7gPWuft3U9pHp4x2DbAmGF4KXGdm+WY2GZgK/IXkjtupZjbZzPKA64JxRUQkTTqzpT8XuAFYbWYrg7avAdeb2QySXTSbgU8DuPtaM3uY5A7aJuBz7t4MYGafB54A4sD97r62F9+LiEirGhsbqamp4ejRo2GX0qsGDBjAuHHjyM3N7fQ0HR6yGaaKigrXjlwR6alNmzZRWFhIcXExyc6L/s/d2bVrFwcOHGDy5MknPWdmy929orXp9ItcEcl6R48ezarABzAziouLu/ztRaEvIpGQTYHfojvvKStDf9/hRr7/1AZer9kbdikiIhklK0M/FoPvPfUWL1btCrsUEREACgoKwi4ByNLQLxyQS2lhPhvrDoZdiohIRsnK0AcoKxlMtUJfRDKMu3PLLbdwzjnncO655/LQQw8BUFtby7x585gxYwbnnHMOzz//PM3NzXz84x8/Pu73vve9Hs+/S+fe6U/KSgv4w5rasMsQkQzzj79byxvb9vfqa04fM4Tb3392p8Z99NFHWblyJatWraK+vp6LLrqIefPm8ctf/pIFCxZw22230dzczOHDh1m5ciVbt25lzZrkb1/37u35fsqs3dIvLx3M3sON7D7UEHYpIiLHvfDCC1x//fXE43FGjhzJZZddxrJly7jooov42c9+xh133MHq1aspLCykrKyM6upqvvCFL/DHP/6RIUOG9Hj+WbulX16a3GlSXXeQosFFIVcjIpmis1vkfaWtH8TOmzeP5557jscee4wbbriBW265hRtvvJFVq1bxxBNP8KMf/YiHH36Y+++/v0fzz9ot/bLSwQDamSsiGWXevHk89NBDNDc3U1dXx3PPPcesWbN4++23GTFiBJ/61Ke46aabWLFiBfX19SQSCa699lq+8Y1vsGLFih7PP2u39McNH0RePEZ13aGwSxEROe6aa67h5Zdf5vzzz8fM+OY3v8moUaNYsmQJ3/rWt8jNzaWgoIAHHniArVu38olPfIJEIgHAnXfe2eP5Z/W5d6747n8zsXgwP13U6ikoRCQi1q1bx7Rp08Iuo0+09t4ie+6d8tICquvVvSMi0iKrQ7+sdDDv7DpMY3Mi7FJERDJClod+AU0J553dh8MuRURClsld2d3VnfeU5aGfPIJHO3NFom3AgAHs2rUrq4K/5Xz6AwYM6NJ0WXv0DkB5yYlj9WFkuMWISGjGjRtHTU0NdXV1YZfSq1qunNUVWR36QwflUlKQpy19kYjLzc097epSUZXV3TsAZSUF+oGWiEgg+0O/dDDV9drSFxGBiIT+7kMN7D2sE6+JiGR96LeceG2j+vVFRLI/9MuOh7769UVEsj70xw8fSG7cdASPiAgRCP2ceIwJRYN06UQRESIQ+tBy4jVt6YuIRCL0y0oLeHvXIZp04jURibiIhP5gGpudLXuOhF2KiEioIhH65cdPvKZ+fRGJtkiEftnxE6+pX19Eoi0SoT98cB5Fg/N0FS0RibxIhD5AWclg/SpXRCIvOqFfOlh9+iISeR2GvpmNN7M/m9k6M1trZl8M2ovM7Ekz2xDcDw/azcx+YGZVZva6mc1Mea1FwfgbzGxR372t000ZUUD9wQZ2H9KJ10Qkujqzpd8EfNndpwGXAJ8zs+nArcDT7j4VeDp4DHAVMDW4LQbuhuRKArgduBiYBdzesqJIh2mjhwCwvnZ/umYpIpJxOgx9d6919xXB8AFgHTAWWAgsCUZbAnwwGF4IPOBJrwDDzGw0sAB40t13u/se4Engyl59N+04a1Qy9N9Q6ItIhHWpT9/MJgEXAK8CI929FpIrBmBEMNpYYEvKZDVBW1vtp85jsZlVmlllb17PsrQwn5KCfNbVHui11xQR6W86HfpmVgA8Atzs7u1tLlsrbd5O+8kN7ve4e4W7V5SWlna2vE6ZNrqQ9du1pS8i0dWp0DezXJKB/wt3fzRo3hF02xDc7wzaa4DxKZOPA7a1054200cPYcOOgzTqHDwiElGdOXrHgPuAde7+3ZSnlgItR+AsAn6b0n5jcBTPJcC+oPvnCWC+mQ0PduDOD9rSZtroITQ0J/TLXBGJrJxOjDMXuAFYbWYrg7avAXcBD5vZTcA7wIeD5x4HrgaqgMPAJwDcfbeZfQNYFoz3dXff3SvvopNajuBZV7ufM0cVpnPWIiIZocPQd/cXaL0/HuDyVsZ34HNtvNb9wP1dKbA3lZUOJi8nxpqt+/jgBaftQxYRyXqR+UUuQG48xnljh1L59p6wSxERCUWkQh/goslFrNm6jyMNzWGXIiKSdpEL/VmTimhKOK9t0da+iERP5EJ/5sThmMGyTQp9EYmeyIX+0IG5nDmykGWb03rgkIhIRohc6APMmlzEinf26ELpIhI5kQ39ww3NvL51X9iliIikVSRDf3ZZMQAvb9wVciUiIukVydAvLsjnrFGFvLSxPuxSRETSKpKhDzCnvITKzXs42qjj9UUkOiIc+sUca0qw4h0duiki0RHZ0L+4rIh4zNSvLyKREtnQLxyQy7ljh/KSQl9EIiSyoQ/JLp5VW/Zy8FhT2KWIiKRFpEN/7pQSmhLOsk36da6IREOkQ//CicPJi8d06KaIREakQ39AbpyZE4epX19EIiPSoQ/J4/XfqN3PnkMNYZciItLnIh/6c6cU4w6vVGtrX0SyX+RD/7xxwxiUF1cXj4hEQuRDPzceY9bkIl7UzlwRiYDIhz7A3PISqusOsX3f0bBLERHpUwp9YHZ5cKrlam3ti0h2U+gD00cPYejAXF6qUr++iGQ3hT4Qixmzy4p5aeMu3D3sckRE+oxCPzB3SjFb9x7hnd2Hwy5FRKTPKPQDs8tLAHTopohkNYV+oLx0MCMK8xX6IpLVFPoBM2NOeTEvb6xXv76IZC2Ffoo5U0qoP9jAWzsOhl2KiEifUOinmBMcr69TLYtItlLopxg3fBATigbxoo7XF5Es1WHom9n9ZrbTzNaktN1hZlvNbGVwuzrlua+aWZWZvWlmC1Larwzaqszs1t5/K71j7pRiXq3eRVNzIuxSRER6XWe29H8OXNlK+/fcfUZwexzAzKYD1wFnB9P82MziZhYHfgRcBUwHrg/GzTizy0s4cKyJtdv2h12KiEiv6zD03f05oLMXkV0IPOjux9x9E1AFzApuVe5e7e4NwIPBuBlndllLv766eEQk+/SkT//zZvZ60P0zPGgbC2xJGacmaGur/TRmttjMKs2ssq6urgfldU9pYT5njCzQzlwRyUrdDf27gXJgBlALfCdot1bG9XbaT290v8fdK9y9orS0tJvl9cyc8hKWbd7NsabmUOYvItJXuhX67r7D3ZvdPQHcS7L7BpJb8ONTRh0HbGunPSPNKS/maGOCle/sDbsUEZFe1a3QN7PRKQ+vAVqO7FkKXGdm+WY2GZgK/AVYBkw1s8lmlkdyZ+/S7pfdty4uKyZm6tcXkeyT09EIZvYr4N1AiZnVALcD7zazGSS7aDYDnwZw97Vm9jDwBtAEfM7dm4PX+TzwBBAH7nf3tb3+bnrJ0IG5nDt2KC9trOd/XXFG2OWIiPSaDkPf3a9vpfm+dsb/Z+CfW2l/HHi8S9WFaHZ5CT99vprDDU0MyutwMYmI9Av6RW4b5pQX05Rwlm3eE3YpIiK9RqHfhopJw8mNGy9V6dBNEckeCv02DMrL4YIJw7UzV0SyikK/HXPKi1mzbR/7DjeGXYqISK9Q6LdjTnkJ7vDKJm3ti0h2UOi3Y8b4YQzMjatfX0SyhkK/HXk5MS6aXKR+fRHJGgr9DswpL2bDzoPsPHA07FJERHpMod+BlksovqytfRHJAgr9Dpw9ZihDBuTwki6hKCJZQKHfgXjMuKSsmJeqtTNXRPo/hX4nzCkvZsvuI2zZfTjsUkREekSh3wlzp5QA6GpaItLvKfQ7YcqIAkoK8nXopoj0ewr9TjAz5pQX89LGXbi3epVHEZF+QaHfSXPKi6k7cIyNdQfDLkVEpNsU+p3U0q//og7dFJF+TKHfSeOLBjFu+EDtzBWRfk2h3wVzyot5pXo3zQn164tI/6TQ74I55SXsO9LIutr9YZciItItCv0uaDkPz4s61bKI9FMK/S4YMWQAU0YU6Hh9Eem3FPpdNKe8mGWbd9PQlAi7FBGRLlPod9Gc8hIONzSzqmZv2KWIiHSZQr+LLikrwgydallE+iWFfhcNG5TH2WOG6Hh9EemXFPrdMKe8hNfe2cuRhuawSxER6RKFfjfMKS+moTlB5du7wy5FRKRLFPrdcNGkInJipkM3RaTfUeh3w+D8HGaMH8ZL+pGWiPQzCv1uetfUUl7fuo/6g8fCLkVEpNMU+t303ukjcIdn1u8MuxQRkU5T6HfT9NFDGDN0AE+9sSPsUkREOq3D0Dez+81sp5mtSWkrMrMnzWxDcD88aDcz+4GZVZnZ62Y2M2WaRcH4G8xsUd+8nfQxMy6fNpLnN9RztFGHbopI/9CZLf2fA1ee0nYr8LS7TwWeDh4DXAVMDW6LgbshuZIAbgcuBmYBt7esKPqzy6eN4EhjMy/rKB4R6Sc6DH13fw449YD0hcCSYHgJ8MGU9gc86RVgmJmNBhYAT7r7bnffAzzJ6SuSfmd2eTGD8+I8uU5dPCLSP3S3T3+ku9cCBPcjgvaxwJaU8WqCtrbaT2Nmi82s0swq6+rqulleeuTnxJl3RilPr9uBu66mJSKZr7d35Forbd5O++mN7ve4e4W7V5SWlvZqcX3h8mkj2bH/GGu26mpaIpL5uhv6O4JuG4L7luMWa4DxKeONA7a1097vvefMUmIGT6mLR0T6ge6G/lKg5QicRcBvU9pvDI7iuQTYF3T/PAHMN7PhwQ7c+UFbv1dckM+FE4fzpA7dFJF+oDOHbP4KeBk408xqzOwm4C7gCjPbAFwRPAZ4HKgGqoB7gc8CuPtu4BvAsuD29aAtK8yfPoo3avezuf5Q2KWIiLTLMnkHZEVFhVdWVoZdRoe27T3CnLue4ctXnMEXLp8adjkiEnFmttzdK1p7Tr/I7QVjhg1k1qQilq7apqN4RCSjKfR7yftnjGHDzoOs334g7FJERNqk0O8lV58zinjM+M1rW8MuRUSkTQr9XlJckM9fnTWCR1ZspbE5EXY5IiKtUuj3oo9UjKf+4DGefTOzf0ksItGl0O9F7z6zlJKCfB6u3NLxyCIiIVDo96LceIxrLxzLM+t3sn3f0bDLERE5jUK/l3101kQS7vzi1bfDLkVE5DQK/V42oXgQl581gl/95R2ONeniKiKSWRT6fWDRnEnUH2zg96tqwy5FROQkCv0+cOmUEqaOKODe56v1C10RySgK/T5gZnzmsnLWbz/AM+t3djyBiEiaKPT7yAdmjGHssIH8+NmN2toXkYyh0O8jufEYn76sjOVv7+GFqvqwyxERART6fepvLhrP2GED+eYf39TWvohkBIV+H8rPiXPze6eyeus+/rBme9jliIgo9Pvah2aOY+qIAr79pzdp0onYRCRkCv0+Fo8ZX55/JtV1h3hkRU3Y5YhIxCn002DB2SM5f/ww/u2pDRxt1K90RSQ8Cv00MDNuvfIsavcd5Z7nqsMuR0QiTKGfJrPLi7n63FH8+NkqavYcDrscEYkohX4a3fY/pgPwL4+vC7kSEYkqhX4ajR02kM++ewqPr97OS/rBloiEQKGfZovnlTFu+EDu+N1aXUtXRNJOoZ9mA3Lj/N/3TeetHQe574VNYZcjIhGj0A/B/OkjmT99JN978i021x8KuxwRiRCFfgjMjK8vPIe8eIzbfrNa5+URkbRR6Idk1NABfOWqs3ixahf/tVy/1BWR9FDoh+ijsyZQMXE4//TYOuoOHAu7HBGJAIV+iGIx465rz+VIYzNfffR1dfOISJ9T6IdsyohCvrLgTJ5at5P/rFQ3j4j0LYV+Bvjk3MlcUlbEP/5uLVt26xQNItJ3ehT6ZrbZzFab2UozqwzaiszsSTPbENwPD9rNzH5gZlVm9rqZzeyNN5ANYjHj2x8+n5gZX354Fc0JdfOISN/ojS3997j7DHevCB7fCjzt7lOBp4PHAFcBU4PbYuDuXph31hg3fBC3f+Bs/rJ5N/e9oDNxikjf6IvunYXAkmB4CfDBlPYHPOkVYJiZje6D+fdb184cy/zpI/n2E2+xfvv+sMsRkSzU09B34E9mttzMFgdtI929FiC4HxG0jwW2pExbE7SdxMwWm1mlmVXW1dX1sLz+xcy480PnMmRgDjc/uFIXXBGRXtfT0J/r7jNJdt18zszmtTOutdJ2Wue1u9/j7hXuXlFaWtrD8vqf4oJ8vvXh81m//QBf//0bYZcjIlmmR6Hv7tuC+53Ar4FZwI6Wbpvgfmcweg0wPmXyccC2nsw/W73nzBF8+rIyfvnqOzyiX+uKSC/qduib2WAzK2wZBuYDa4ClwKJgtEXAb4PhpcCNwVE8lwD7WrqB5HR/P/9MZpcV89VHV1O5eXfY5YhIlujJlv5I4AUzWwX8BXjM3f8I3AVcYWYbgCuCxwCPA9VAFXAv8NkezDvr5cZj3P2xmYwdPpDF/7Fcx++LSK+wTP7pf0VFhVdWVoZdRqiq6w5yzY9fYuSQfB75uzkUDsgNuyQRyXBmtjzlMPqT6Be5Ga6stIC7PzqT6rpDfOFXr9Gkq22JSA8o9PuBOVNK+PrCc3j2zTr+6TFdVF1Eui8n7AKkc/724glsrEteYnHKiAI+dsnEsEsSkX5Iod+PfO3qaWyqP8TtS9cyqXgwl04tCbskEeln1L3Tj8Rjxvevm8GU0gL+7hfLqdp5MOySRKSfUej3M4UDcvnpogryc2LctGQZew41hF2SiPQjCv1+aHzRIH5yQwW1+47ymf+3nIYmHdEjIp2j0O+nLpw4nG/99Xm8umk3/+c3q3WpRRHpFO3I7ccWzhjLxp0H+cEzVUwZUcDieeVhlyQiGU6h38/d/N4z2Fh3iDv/sJ7JJQVcMX1k2CWJSAZT904/13KpxfPGDuWLD77G2m37wi5JRDKYQj8LDMyLc++NFQwdmMunllSy88DRsEsSkQyl0M8SI4YM4KeLKth7pJFPPbBcV90SkVYp9LPI2WOG8m9/M4PXa/Zyy3+9riN6ROQ0Cv0sM//sUXxlwVn8btU2fvhMVdjliEiG0dE7Wegzl5WxYecBvvvkW5SVDuZ9540JuyQRyRDa0s9CZsadHzqXionD+fLDq1i1ZW/YJYlIhlDoZ6n8nDg/ueFCSgvz+dQDlWzfpyN6REShn9WKC/K5b9FFHDrWxP98YBlHGnREj0jUKfSz3JmjCvnh317A2m37+dLDK0kkdESPSJQp9CPgr84ayW1XT+MPa7bzD0vX6FBOkQjT0TsRcdOlk6k7cIyfPFdNY5PzD++fzuB8/flFokb/9RFhZtx61VnEY8aPn93IixvruetD5+mSiyIRo+6dCDEzvnLlWTz86dnkxWN87L5X+dJDK9my+3DYpYlImlgm9+9WVFR4ZWVl2GVkpaONzfzwmQ3c+/wm3J0rpo/kry8cx7yppeTEtS0g0p+Z2XJ3r2j1OYV+tG3fd5R7nqvmNyu3svtQA6WF+Vx59igWnD2Ki8uKyNUKQKTfUehLhxqaEvz5zZ38esVWnn1rJ0cbEwwZkMPl00by3mkjmTulmGGD8sIuU0Q6ob3Q145cASAvJ8aCYAv/SEMzz2+o409v7OCpdTv49WtbiRmcO3Yol04t4dIppVw4cTh5OfoWINLfaEtf2tXUnGDllr08v6GeF6rqWbllL80JZ2BunIvLirh0SgnvmlrKGSMLMLOwyxUR1L0jvWj/0UZe2biLF6rqeWFDPdX1hwAYUZjPpVNKmDOlhNnlxYwdNjDkSkWiS6EvfWbr3iO8sKGO5zfU82JVPXsONwIwoWgQl5QVMbu8mNllJYwaOiDkSkWiQ6EvaZFIOG/uOMDLG3fxSvUuXt20m31HkiuBscMGUlY6mGGD8hiUG2dgXpxBeXEGBsMDcuPk5cTIP3478Tiv1cfJ+7x4TN1KIqfQjlxJi1jMmDZ6CNNGD+GTl06mOeGs376flzfuYs3WfVTXH6JmzxEONzRxuKGZIw3NNPXCCeDaX1nEyI2fuOXl2MmP48HjnFMeB20nPW51+hi5QVve8XYLpk0+jse0UpLMkfbQN7Mrge8DceCn7n5XumuQ9IjHjLPHDOXsMUPbHKehKcGRhmaONTVzrCnBsaYEDU0JjjU109CUoKE5wbHG4D5oOzHO6eO39rixOcGhhmYag+HkzWloGW468bgvxIzjK4l4zMiNG/GYkROLkRM3ck4dThkvJxYL2k6ME48ZubEY8biRGzPisdiJ14zHiBnEzIhZ8lfY8ZgdbzOzk56PxeykcWOpz8fstNeKmRGPnT6uHR+vZVyAE+0Gx4cJhg0jFkveJ587MY1x4nVbnrfjNaS0kWxrbRqC17Rg/i3PcdJ4Ka8ZkW+MaQ19M4sDPwKuAGqAZWa21N3fSGcdkjnygi1yyA27FNydpoQHK4KUlUJwa2jyE8PBiqNlRdLyuCFlxdIQvE7L46aE03T8PjmvpsSJ9uaE09gctDU7RxsTNCWaU547MU7zSdOeeK43vjlFXeqKwk4ZPrECO+X5U1ZuJz1/ygqK4+O0svICgvUV08cM5YfXX9Dr7y/dW/qzgCp3rwYwsweBhYBCX0Jnlty6zo3HoB//Ds3dSTgkPLly8GA4EbS3PJ987sS4CT8xbnPi5HGPT59o/bWSzwVtnHjOAYLn3cE5MQwt80tO0zLv5HsIXieRnMaPT3/6NCc931JX0AZtzTu5D8pT5xU88FOm8ZT3cvz1U+fvJ14nkTJ8as2p8yK17uPtJy+zCUV9cwRcukN/LLAl5XENcHHqCGa2GFgMMGHChPRVJpIlzIy4QRwjNx52NZJp0v2TytY6zU76Puru97h7hbtXlJaWpqksEZFoSHfo1wDjUx6PA7aluQYRkchKd+gvA6aa2WQzywOuA5amuQYRkchKa5++uzeZ2eeBJ0gesnm/u69NZw0iIlGW9uP03f1x4PF0z1dERHS5RBGRSFHoi4hEiEJfRCRCMvosm2ZWB7zdg5coAep7qZzepLq6JlPrgsytTXV1TabWBd2rbaK7t/pDp4wO/Z4ys8q2Ti8aJtXVNZlaF2RubaqrazK1Luj92tS9IyISIQp9EZEIyfbQvyfsAtqguromU+uCzK1NdXVNptYFvVxbVvfpi4jIybJ9S19ERFIo9EVEIiQrQ9/MrjSzN82sysxuDbGO8Wb2ZzNbZ2ZrzeyLQfsdZrbVzFYGt6tDqm+zma0OaqgM2orM7Ekz2xDcD09zTWemLJeVZrbfzG4OY5mZ2f1mttPM1qS0tbp8LOkHwWfudTObmea6vmVm64N5/9rMhgXtk8zsSMpy+/e+qqud2tr825nZV4Nl9qaZLUhzXQ+l1LTZzFYG7WlbZu1kRN99zpKX9MqeG8mzd24Eykhe9G4VMD2kWkYDM4PhQuAtYDpwB/D3GbCsNgMlp7R9E7g1GL4V+NeQ/5bbgYlhLDNgHjATWNPR8gGuBv5A8kJBlwCvprmu+UBOMPyvKXVNSh0vpGXW6t8u+F9YBeQDk4P/23i66jrl+e8A/5DuZdZORvTZ5ywbt/SPX4fX3RuAluvwpp2717r7imD4ALCO5CUjM9lCYEkwvAT4YIi1XA5sdPee/Cq729z9OWD3Kc1tLZ+FwAOe9AowzMxGp6sud/+TuzcFD18heYGitGtjmbVlIfCgux9z901AFcn/37TWZcmrlX8E+FVfzLs97WREn33OsjH0W7sOb+hBa2aTgAuAV4Omzwdfz+5PdxdKCgf+ZGbLLXltYoCR7l4LyQ8kMCKk2iB5kZ3Uf8RMWGZtLZ9M+tx9kuTWYIvJZvaamf23mb0rpJpa+9tlyjJ7F7DD3TektKV9mZ2SEX32OcvG0O/wOrzpZmYFwCPAze6+H7gbKAdmALUkv1qGYa67zwSuAj5nZvNCquM0lryy2geA/wyaMmWZtSUjPndmdhvQBPwiaKoFJrj7BcCXgF+a2ZA0l9XW3y4jlhlwPSdvXKR9mbWSEW2O2kpbl5ZZNoZ+Rl2H18xySf4xf+HujwK4+w53b3b3BHAvffSVtiPuvi243wn8OqhjR8vXxeB+Zxi1kVwRrXD3HUGNGbHMaHv5hP65M7NFwPuAj3rQARx0newKhpeT7Dc/I511tfO3y4RllgN8CHiopS3dy6y1jKAPP2fZGPoZcx3eoK/wPmCdu383pT21D+4aYM2p06ahtsFmVtgyTHJH4BqSy4Zb8lIAAAEoSURBVGpRMNoi4Lfpri1w0tZXJiyzQFvLZylwY3B0xSXAvpav5+lgZlcC/xv4gLsfTmkvNbN4MFwGTAWq01VXMN+2/nZLgevMLN/MJge1/SWdtQHvBda7e01LQzqXWVsZQV9+ztKxhzrdN5J7uN8iuYa+LcQ6LiX51et1YGVwuxr4D2B10L4UGB1CbWUkj5xYBaxtWU5AMfA0sCG4LwqhtkHALmBoSlvalxnJlU4t0EhyC+umtpYPya/dPwo+c6uBijTXVUWyr7flc/bvwbjXBn/fVcAK4P0hLLM2/3bAbcEyexO4Kp11Be0/Bz5zyrhpW2btZESffc50GgYRkQjJxu4dERFpg0JfRCRCFPoiIhGi0BcRiRCFvohIhCj0RUQiRKEvIhIh/x9OjEPwfOSocgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = pd.DataFrame(model.history.history)\n",
    "l.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
